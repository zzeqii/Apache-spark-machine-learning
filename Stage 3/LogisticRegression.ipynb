{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"Python Spark LR Classifier\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and parse the data file,converitn it to a DataFrame\n",
    "#path='hdfs://soit-hdp-pro-1.ucc.usyd.edu.au/share/MNIST/'\n",
    "path='../Data/'\n",
    "training_data=spark.read.csv(path+'Train-label-28x28.csv', header=False, inferSchema=\"true\").withColumnRenamed('_c0','label')\n",
    "testing_data=spark.read.csv(path+'Test-label-28x28.csv',header=False, inferSchema=\"true\").withColumnRenamed('_c0','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+\n",
      "|label|            features| ID|\n",
      "+-----+--------------------+---+\n",
      "|    5|(784,[152,153,154...|  0|\n",
      "|    0|(784,[127,128,129...|  1|\n",
      "|    4|(784,[160,161,162...|  2|\n",
      "|    1|(784,[158,159,160...|  3|\n",
      "|    9|(784,[208,209,210...|  4|\n",
      "+-----+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#Train Data\n",
    "assembler=VectorAssembler(inputCols=training_data.columns[1:],outputCol='features')\n",
    "newdata=assembler.transform(training_data)\n",
    "train_data=newdata.select('label','features')\n",
    "train_id = train_data.withColumn(\n",
    "        '{}_id'.format(train_data), monotonically_increasing_id())\n",
    "training=train_id.withColumnRenamed('DataFrame[label: int, features: vector]_id','ID')\n",
    "training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+\n",
      "|label|            features| ID|\n",
      "+-----+--------------------+---+\n",
      "|    7|(784,[202,203,204...|  0|\n",
      "|    2|(784,[94,95,96,97...|  1|\n",
      "|    1|(784,[128,129,130...|  2|\n",
      "|    0|(784,[124,125,126...|  3|\n",
      "|    4|(784,[150,151,159...|  4|\n",
      "+-----+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Data\n",
    "assembler_test=VectorAssembler(inputCols=testing_data.columns[1:],outputCol='features')\n",
    "newdata_test=assembler_test.transform(testing_data)\n",
    "test_data=newdata_test.select('label','features')\n",
    "test_id = test_data.withColumn(\n",
    "        '{}_id'.format(test_data), monotonically_increasing_id())\n",
    "testing=test_id.withColumnRenamed('DataFrame[label: int, features: vector]_id','ID')\n",
    "testing.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+\n",
      "|label|             feature| ID|\n",
      "+-----+--------------------+---+\n",
      "|    5|[880.731433034386...|  0|\n",
      "|    0|[1768.51722024166...|  1|\n",
      "|    4|[704.949236329314...|  2|\n",
      "|    1|[-42.328192193772...|  3|\n",
      "|    9|[374.043902028333...|  4|\n",
      "+-----+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#apply PCA\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "\n",
    "pca = PCA(k=50, inputCol=\"features\", outputCol=\"feature\")\n",
    "pca_train = pca.fit(training)\n",
    "\n",
    "#Apply PCA to train / test features\n",
    "train_pca = pca_train.transform(training).select(\"label\",\"feature\",\"ID\")\n",
    "test_pca = pca_train.transform(testing).select(\"label\",\"feature\",\"ID\")\n",
    "train_pca.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the LR model\n",
    "lr = LogisticRegression(featuresCol=\"features\",family=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 ms, sys: 4.51 ms, total: 16.9 ms\n",
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.47 ms, sys: 2.74 ms, total: 11.2 ms\n",
      "Wall time: 60.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#train the model\n",
    "prediction=lrModel.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+--------------------+----------+\n",
      "|label|            features| ID|         probability|prediction|\n",
      "+-----+--------------------+---+--------------------+----------+\n",
      "|    7|(784,[202,203,204...|  0|[9.64530396645497...|       7.0|\n",
      "+-----+--------------------+---+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select columns\n",
    "prediction.select('label','features','ID','probability','prediction').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9269\n",
      "Test Error = 0.0731\n",
      "CPU times: user 11.4 ms, sys: 7.38 ms, total: 18.8 ms\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Select (prediction, true label) and compute test error\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients and intercept for multinomial logistic regression\n",
    "# print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\n",
    "# print(\"Intercept: \" + str(lrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "def prepare_data(actual_data, prediction_data, on='ID'):\n",
    "    return actual_data.join(prediction_data, on).rdd \\\n",
    "        .map(lambda x: (float(x.prediction), float(x.label)))\n",
    "\n",
    "\n",
    "def overall_report(actual_data, prediction_data):\n",
    "    # Calculate actual / predicted labels in rdd from\n",
    "    prediction_and_labels = prepare_data(actual_data, prediction_data)\n",
    "\n",
    "    # Calculate actual / predicted labels in rdd from\n",
    "    metrics = MulticlassMetrics(prediction_and_labels)\n",
    "\n",
    "    # Calculate overall level metrics\n",
    "    # print('Precision:', metrics.precision(), type(metrics.precision()))\n",
    "    # return sc.parallelize([(Vectors.dense(metrics.accuracy),\n",
    "    #                         Vectors.dense(metrics.precision()),\n",
    "    #                         Vectors.dense(metrics.recall()),\n",
    "    #                         Vectors.dense(metrics.fMeasure()))]).toDF(['Accuracy', 'Precision', 'Recall', 'F - Score'])\n",
    "    print('Accuracy\\tPrecision\\tRecall\\tF-Score')\n",
    "    print('{}\\t{}\\t{}\\t{}'.format(metrics.accuracy, metrics.precision(),\n",
    "                                  metrics.recall(), metrics.fMeasure()))\n",
    "\n",
    "\n",
    "def classification_report(actual_data, prediction_data):\n",
    "    # Calculate actual / predicted labels in rdd from\n",
    "    prediction_and_labels = prepare_data(actual_data, prediction_data)\n",
    "\n",
    "    # Calculate calculate class level metrics\n",
    "    metrics = MulticlassMetrics(prediction_and_labels)\n",
    "    classes = set(actual_data.rdd.map(lambda x: float(x.label)).collect())\n",
    "    print('Class\\tPrecision\\tRecall\\tF-Score')\n",
    "    for c in sorted(classes):\n",
    "        print('{}\\t{}\\t{}\\t{}'.format(c,\n",
    "                                      round(metrics.precision(c), 3),\n",
    "                                      round(metrics.recall(c), 3),\n",
    "                                      round(metrics.fMeasure(c), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\tPrecision\tRecall\tF-Score\n",
      "0.9269\t0.9269\t0.9269\t0.9269\n"
     ]
    }
   ],
   "source": [
    "#print out precision, recall, f1-score\n",
    "overall_report(test_pca, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\tPrecision\tRecall\tF-Score\n",
      "0.0\t0.957\t0.973\t0.965\n",
      "1.0\t0.964\t0.98\t0.972\n",
      "2.0\t0.934\t0.892\t0.913\n",
      "3.0\t0.898\t0.924\t0.911\n",
      "4.0\t0.938\t0.934\t0.936\n",
      "5.0\t0.914\t0.869\t0.891\n",
      "6.0\t0.94\t0.955\t0.948\n",
      "7.0\t0.927\t0.925\t0.926\n",
      "8.0\t0.878\t0.888\t0.883\n",
      "9.0\t0.913\t0.918\t0.915\n"
     ]
    }
   ],
   "source": [
    "# print out precision, recall, f1-score for each class\n",
    "classification_report = classification_report(test_pca, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
