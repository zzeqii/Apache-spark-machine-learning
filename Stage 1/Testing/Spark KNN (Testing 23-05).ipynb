{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "from math import sqrt\n",
    "from pyspark.sql.functions import sqrt\n",
    "\n",
    "# Data \n",
    "train_data_labels = 'test/Train-label-28x28-2.csv'\n",
    "test_data_labels = 'test/Test-label-28x28-2.csv'\n",
    "\n",
    "def get_vector(data, col_name):\n",
    "    assembler = VectorAssembler(inputCols=data.columns, outputCol=col_name)\n",
    "    return assembler.transform(data).select(col_name)    \n",
    "\n",
    "# Train Data\n",
    "train = spark.read.csv(train_data_labels, header=False, inferSchema=\"true\")\n",
    "train_labels = get_vector(train.select('_c0'), 'train_labels')\n",
    "train_features = get_vector(train.drop('_c0'), 'features')\n",
    "\n",
    "# Test Data\n",
    "test = spark.read.csv(test_data_labels, header=False, inferSchema=\"true\")\n",
    "test_labels = get_vector(test.select('_c0'), 'test_labels')\n",
    "test_features = get_vector(test.drop('_c0'), 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=50, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(train_features)\n",
    "\n",
    "# Apply PCA to train / test features\n",
    "train_features_pca = pca_model.transform(train_features).select(\"pca_features\")\n",
    "test_features_pca = pca_model.transform(test_features).select(\"pca_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename pca feature column values\n",
    "train_features_pca = train_features_pca.withColumnRenamed(\"pca_features\", \"train_features\")\n",
    "test_features_pca = test_features_pca.withColumnRenamed(\"pca_features\", \"test_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop a combined dataframe for all data\n",
    "\n",
    "def combine_features_labels(feature_vector, label_vector, kind='train'):\n",
    "    features = feature_vector.withColumn('{}_id'.format(kind), monotonically_increasing_id())\n",
    "    labels = label_vector.withColumn('{}_id'.format(kind), monotonically_increasing_id())\n",
    "    data = features.join(labels, '{}_id'.format(kind))\n",
    "    return data\n",
    "\n",
    "# Create combined train / test data\n",
    "train_data = combine_features_labels(train_features_pca, train_labels, 'train')\n",
    "test_data = combine_features_labels(test_features_pca, test_labels, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+\n",
      "|train_id|      train_features|train_labels|\n",
      "+--------+--------------------+------------+\n",
      "|       0|[850.672141291628...|       [5.0]|\n",
      "|       1|[1699.40798562470...|       [0.0]|\n",
      "|       2|[632.202682471733...|       [4.0]|\n",
      "|       3|[-166.42740423231...|       [1.0]|\n",
      "+--------+--------------------+------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+-------+--------------------+-----------+\n",
      "|test_id|       test_features|test_labels|\n",
      "+-------+--------------------+-----------+\n",
      "|      0|[361.419311328079...|      [7.0]|\n",
      "|      1|[873.283433767456...|      [2.0]|\n",
      "|      2|[-197.20339527162...|      [1.0]|\n",
      "|      3|[1906.37377339178...|      [0.0]|\n",
      "+-------+--------------------+-----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, 600, 100)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.show(4), test_data.show(4), train_data.count(), test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_data.rdd.map(lambda x: x.test_features).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = sc.broadcast(train_data.rdd.map(lambda x: x.train_features).collect())\n",
    "train_labels = sc.broadcast(train_data.rdd.map(lambda x: x.train_labels).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of total examples\n",
    "num_examples = len(train_labels.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "K = 10\n",
    "\n",
    "def cosine_distance(vec_one, vec_two):\n",
    "    vec_one_sq = np.sqrt(vec_one.dot(vec_one))\n",
    "    vec_two_sq = np.sqrt(vec_two.dot(vec_two))\n",
    "    vec_one_two_dot = vec_one.dot(vec_two)\n",
    "    similarity = vec_one_two_dot / (vec_one_sq * vec_two_sq)\n",
    "    return float(similarity)\n",
    "\n",
    "def majority_vote(sorted_dist):\n",
    "    closest_instance = sorted_dist[0][1]\n",
    "    neighbors = [neighbor[1] for neighbor in sorted_dist]\n",
    "    counts = {label: neighbors.count(label) for label in neighbors}\n",
    "    most_common_vote = max(counts.items(), key=operator.itemgetter(1))\n",
    "    if most_common_vote[1] > 1:\n",
    "        return float(most_common_vote[0])\n",
    "    else:\n",
    "        return float(closest_instance)\n",
    "    \n",
    "def compute_label(test_id, test_label, test_feature):\n",
    "    results = []\n",
    "    for i in range(num_examples):\n",
    "        train_feature = train_features.value[i]\n",
    "        train_label = train_labels.value[i]\n",
    "        similarity = cosine_distance(train_feature, test_feature)\n",
    "        results.append((similarity, train_label[0]))\n",
    "    sorted_dist = sorted(results, reverse=True)[:K]\n",
    "    prediction = majority_vote(sorted_dist)\n",
    "    return (test_id, Vectors.dense(prediction))\n",
    "\n",
    "# Compute KNN predictions\n",
    "result = test_data.rdd.map(lambda x: compute_label(x.test_id, x.test_labels, x.test_features)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of prediction resutlts\n",
    "prediction_data = sc.parallelize(result).toDF(['test_id', 'prediction'])\n",
    "prediction_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+\n",
      "|test_id|       test_features|test_labels|\n",
      "+-------+--------------------+-----------+\n",
      "|      0|[361.419311328079...|      [7.0]|\n",
      "|      1|[873.283433767456...|      [2.0]|\n",
      "|      2|[-197.20339527162...|      [1.0]|\n",
      "+-------+--------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+----------+\n",
      "|test_id|prediction|\n",
      "+-------+----------+\n",
      "|      0|     [7.0]|\n",
      "|      1|     [2.0]|\n",
      "|      2|     [1.0]|\n",
      "+-------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input dataframes to calculate summary statistics\n",
    "test_data.show(3), prediction_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics\n",
      "\n",
      "Precision\tRecall\tF-Score\n",
      "0.85\t0.85\t0.85\n",
      "\n",
      "Class Summary Statistics\n",
      "\n",
      "Class\tPrecision\tRecall\tF-Score\n",
      "0.0\t0.8\t1.0\t0.889\n",
      "1.0\t0.933\t1.0\t0.966\n",
      "2.0\t0.778\t0.875\t0.824\n",
      "3.0\t1.0\t0.909\t0.952\n",
      "4.0\t1.0\t0.714\t0.833\n",
      "5.0\t0.833\t0.714\t0.769\n",
      "6.0\t0.889\t0.8\t0.842\n",
      "7.0\t0.8\t0.8\t0.8\n",
      "8.0\t1.0\t0.5\t0.667\n",
      "9.0\t0.667\t0.909\t0.769\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Calculation of statistics\n",
    "\n",
    "def summary_statistics(metrics):\n",
    "    # Overall statistics\n",
    "    precision = metrics.precision()\n",
    "    recall = metrics.recall()\n",
    "    f1Score = metrics.fMeasure()\n",
    "    print('Precision\\tRecall\\tF-Score')\n",
    "    print('{}\\t{}\\t{}'.format(metrics.precision(), metrics.recall(), metrics.fMeasure()))\n",
    "    \n",
    "\n",
    "def label_statistics(metrics, labels):\n",
    "    print('Class\\tPrecision\\tRecall\\tF-Score')\n",
    "    for label in sorted(labels):\n",
    "        print('{}\\t{}\\t{}\\t{}'.format(label, \n",
    "                                      round(metrics.precision(label), 3), \n",
    "                                      round(metrics.recall(label), 3), \n",
    "                                      round(metrics.fMeasure(label), 3)))\n",
    "\n",
    "        \n",
    "def statistics(test_data, prediction_data):\n",
    "    # Compute raw scores on the test set\n",
    "    prediction_and_labels = test_data.join(prediction_data, 'test_id').rdd \\\n",
    "                            .map(lambda x: (float(x.prediction[0]), float(x.test_labels[0])))\n",
    "\n",
    "    # Instantiate metrics object\n",
    "    metrics = MulticlassMetrics(prediction_and_labels)\n",
    "\n",
    "    # Overall statistics\n",
    "    print(\"Summary Statistics\\n\")\n",
    "    summary_statistics(metrics)\n",
    "\n",
    "    # Statistics by class\n",
    "    print(\"\\nClass Summary Statistics\\n\")\n",
    "    label_statistics(metrics, labels)\n",
    "      \n",
    "\n",
    "statistics(test_data, prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
