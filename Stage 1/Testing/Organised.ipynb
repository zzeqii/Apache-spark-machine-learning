{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Numpy\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "\n",
    "INPUT_TRAIN_DATA = 'test/Train-label-28x28-2.csv'\n",
    "INPUT_TEST_DATA = 'test/Test-label-28x28-2.csv'\n",
    "\n",
    "FILE_FLAG = 'numpy-test'\n",
    "CWD = os.getcwd()\n",
    "OUTPUT_DIR = '{}'.format(CWD)\n",
    "\n",
    "K_NEAREST_NEIGHBORS = 5\n",
    "\n",
    "\n",
    "def get_vector(data, col_name):\n",
    "    assembler = VectorAssembler(inputCols=data.columns, outputCol=col_name)\n",
    "    return assembler.transform(data).select(col_name)    \n",
    "\n",
    "\n",
    "def combine_features_labels(feature_vector, label_vector, kind='train'):\n",
    "    features = feature_vector.withColumn('{}_id'.format(kind), monotonically_increasing_id())\n",
    "    labels = label_vector.withColumn('{}_id'.format(kind), monotonically_increasing_id())\n",
    "    data = features.join(labels, '{}_id'.format(kind))\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_preprocessed_data(input_train, input_test):\n",
    "    # Train Data\n",
    "    train = spark.read.csv(input_train, header=False, inferSchema=\"true\")\n",
    "    train_labels = get_vector(train.select('_c0'), 'train_label')\n",
    "    train_features = get_vector(train.drop('_c0'), 'feature')\n",
    "\n",
    "    # Test Data\n",
    "    test = spark.read.csv(input_test, header=False, inferSchema=\"true\")\n",
    "    test_labels = get_vector(test.select('_c0'), 'test_label')\n",
    "    test_features = get_vector(test.drop('_c0'), 'feature')\n",
    "    \n",
    "    # Compute PCA\n",
    "    pca = PCA(k=50, inputCol=\"feature\", outputCol=\"pca_feature\")\n",
    "    pca_model = pca.fit(train_features)\n",
    "\n",
    "    # Apply PCA to train / test features\n",
    "    train_features_pca = pca_model.transform(train_features).select(\"pca_feature\")\n",
    "    test_features_pca = pca_model.transform(test_features).select(\"pca_feature\")\n",
    "    \n",
    "    # Rename pca feature column values\n",
    "    train_features_pca = train_features_pca.withColumnRenamed(\"pca_feature\", \"train_feature\")\n",
    "    test_features_pca = test_features_pca.withColumnRenamed(\"pca_feature\", \"test_feature\")\n",
    "    \n",
    "    # Create combined train / test data\n",
    "    train_data = combine_features_labels(train_features_pca, train_labels, 'train')\n",
    "    test_data = combine_features_labels(test_features_pca, test_labels, 'test')\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def make_prediction(test_feature, train_features, train_lables, K=3):\n",
    "    # Calulate components\n",
    "    train_features = np.array(train_features.value)\n",
    "    train_lables = np.array(train_lables.value)\n",
    "    test_feature = np.array(test_feature)\n",
    "    \n",
    "    # Calculate similarity with euclidean distance\n",
    "    similarity = ((test_feature - train_features) ** 2).sum(axis=1) ** 0.5\n",
    "    \n",
    "    # Calculate K nearest neighbors\n",
    "    k_neighbors = train_lables[np.argpartition(similarity, K)[:K]]\n",
    "    \n",
    "    # Compute predicted label by majority vote\n",
    "    predicted_label = collections.Counter(k_neighbors.ravel()).most_common()[0][0]\n",
    "    \n",
    "    # Return result\n",
    "    return Vectors.dense(predicted_label)\n",
    "\n",
    "\n",
    "def prepare_data(actual_data, prediction_data, on='test_id'):\n",
    "    return actual_data.join(prediction_data, on).rdd \\\n",
    "                        .map(lambda x: (float(x.prediction[0]), float(x.test_label[0])))\n",
    "\n",
    "    \n",
    "def overall_report(actual_data, prediction_data):\n",
    "    # Calculate actual / predicted labels in rdd from\n",
    "    prediction_and_labels = prepare_data(actual_data, prediction_data)\n",
    "    \n",
    "    # Calculate actual / predicted labels in rdd from\n",
    "    metrics = MulticlassMetrics(prediction_and_labels)\n",
    "    \n",
    "    # Calculate overall level metrics\n",
    "    return sc.parallelize([(Vectors.dense(round(metrics.precision(), 3)), \n",
    "                            Vectors.dense(round(metrics.recall(), 3)), \n",
    "                            Vectors.dense(round(metrics.fMeasure(), 3)))]).toDF(['Precision', 'Recall', 'F-Score'])\n",
    "\n",
    "\n",
    "def classification_report(actual_data, prediction_data):\n",
    "    # Calculate actual / predicted labels in rdd from\n",
    "    prediction_and_labels = prepare_data(actual_data, prediction_data)\n",
    "    \n",
    "    # Calculate calculate class level metrics\n",
    "    metrics = MulticlassMetrics(prediction_and_labels)\n",
    "    classes = set(actual_data.rdd.map(lambda x: x.test_label[0]).collect())\n",
    "    results = [(Vectors.dense(float(c)),\n",
    "                Vectors.dense(round(metrics.precision(c), 3)), \n",
    "                Vectors.dense(round(metrics.recall(c), 3)), \n",
    "                Vectors.dense(round(metrics.fMeasure(c), 3))) for c in sorted(classes)]\n",
    "    return sc.parallelize(results).toDF(['Class', 'Precision', 'Recall', 'F-Score'])\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_data, test_data = get_preprocessed_data(INPUT_TRAIN_DATA, INPUT_TEST_DATA)\n",
    "    \n",
    "    # Broadcast training features and examples\n",
    "    train_features = sc.broadcast(train_data.rdd.map(lambda x: x.train_feature).collect())\n",
    "    train_labels = sc.broadcast(train_data.rdd.map(lambda x: x.train_label).collect())\n",
    "    \n",
    "    # Compute KNN predictions\n",
    "    predictions = test_data.rdd.map(\n",
    "        lambda x: (x.test_id, make_prediction(\n",
    "            x.test_feature, train_features, train_labels, K=K_NEAREST_NEIGHBORS))).collect()\n",
    "\n",
    "    # Create dataframe of prediction resutlts\n",
    "    prediction_data = sc.parallelize(predictions).toDF(['test_id', 'prediction'])\n",
    "    \n",
    "    # Calculate overall summary statistics\n",
    "    overall_metrics = overall_report(test_data, prediction_data)\n",
    "    overall_metrics.show()\n",
    "    \n",
    "    # Calculate class level statistics\n",
    "    classification_metrics = classification_report(test_data, prediction_data)\n",
    "    classification_metrics.show()\n",
    "    \n",
    "    # Save statistics to CSV\n",
    "    overall_metrics.toPandas().to_csv(\n",
    "        '{0}/overall_metrics{1}.csv'.format(OUTPUT_DIR, FILE_FLAG), index=None)\n",
    "    classification_metrics.toPandas().to_csv(\n",
    "        '{0}/classification_report{1}.csv'.format(OUTPUT_DIR, FILE_FLAG), index=None)\n",
    "    \n",
    "    print('Complete. Output saved to {}'.format(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/Users/nicholasmoore/Google Drive/University/COMP5349 - Cloud Computing/Assessment/Assignment 2/Code/Stage 1/Testing/test/Train-label-28x28-2.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.0/libexec/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o31.csv.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/nicholasmoore/Google Drive/University/COMP5349 - Cloud Computing/Assessment/Assignment 2/Code/Stage 1/Testing/test/Train-label-28x28-2.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:715)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:594)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a2837e2b1cb1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_preprocessed_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_TRAIN_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINPUT_TEST_DATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# Broadcast training features and examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a2837e2b1cb1>\u001b[0m in \u001b[0;36mget_preprocessed_data\u001b[0;34m(input_train, input_test)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_preprocessed_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Train Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_c0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_c0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'feature'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.0/libexec/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.0/libexec/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/Users/nicholasmoore/Google Drive/University/COMP5349 - Cloud Computing/Assessment/Assignment 2/Code/Stage 1/Testing/test/Train-label-28x28-2.csv;'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Cross Join\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "# CONFIG --- START ---\n",
    "\n",
    "INPUT_TRAIN_DATA = 'test/Train-label-28x28-2.csv'\n",
    "INPUT_TEST_DATA = 'test/Test-label-28x28-2.csv'\n",
    "\n",
    "FILE_FLAG = 'cross-test'\n",
    "CWD = os.getcwd()\n",
    "OUTPUT_DIR = '{}'.format(CWD)\n",
    "\n",
    "K_NEAREST_NEIGHBORS = 5\n",
    "\n",
    "# CONFIG --- END ---\n",
    "\n",
    "def get_vector(data, col_name):\n",
    "    assembler = VectorAssembler(inputCols=data.columns, outputCol=col_name)\n",
    "    return assembler.transform(data).select(col_name)    \n",
    "\n",
    "\n",
    "def combine_features_labels(feature_vector, label_vector, kind='train'):\n",
    "    features = feature_vector.withColumn('{}_id'.format(kind), monotonically_increasing_id())\n",
    "    labels = label_vector.withColumn('{}_id'.format(kind), monotonically_increasing_id())\n",
    "    data = features.join(labels, '{}_id'.format(kind))\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_preprocessed_data(input_train, input_test):\n",
    "    # Train Data\n",
    "    train = spark.read.csv(input_train, header=False, inferSchema=\"true\")\n",
    "    train_labels = get_vector(train.select('_c0'), 'train_label')\n",
    "    train_features = get_vector(train.drop('_c0'), 'feature')\n",
    "\n",
    "    # Test Data\n",
    "    test = spark.read.csv(input_test, header=False, inferSchema=\"true\")\n",
    "    test_labels = get_vector(test.select('_c0'), 'test_label')\n",
    "    test_features = get_vector(test.drop('_c0'), 'feature')\n",
    "    \n",
    "    # Compute PCA\n",
    "    pca = PCA(k=50, inputCol=\"feature\", outputCol=\"pca_feature\")\n",
    "    pca_model = pca.fit(train_features)\n",
    "\n",
    "    # Apply PCA to train / test features\n",
    "    train_features_pca = pca_model.transform(train_features).select(\"pca_feature\")\n",
    "    test_features_pca = pca_model.transform(test_features).select(\"pca_feature\")\n",
    "    \n",
    "    # Rename pca feature column values\n",
    "    train_features_pca = train_features_pca.withColumnRenamed(\"pca_feature\", \"train_feature\")\n",
    "    test_features_pca = test_features_pca.withColumnRenamed(\"pca_feature\", \"test_feature\")\n",
    "    \n",
    "    # Create combined train / test data\n",
    "    train_data = combine_features_labels(train_features_pca, train_labels, 'train')\n",
    "    test_data = combine_features_labels(test_features_pca, test_labels, 'test')\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def make_prediction(train_data, test_data, K=3):\n",
    "    # Find all cartesian pairs of train and test data\n",
    "    cross = test_data.crossJoin(train_data)\n",
    "    \n",
    "    # Define UDF to calculate euclidean distance\n",
    "    distance_udf = F.udf(lambda x, y: float((np.array(x - y) ** 2).sum() ** 0.5), FloatType())\n",
    "    \n",
    "    # Calculate similarity with euclidean distance\n",
    "    similarity = cross.withColumn(\n",
    "        'distance', distance_udf(F.col('test_feature'), F.col('train_feature')))\n",
    "    \n",
    "    # Define window to sort distance for each test instance\n",
    "    window = Window.partitionBy(similarity.test_id).orderBy(similarity.distance.asc())\n",
    "    \n",
    "    # Calculate K nearest neighbors\n",
    "    k_neighbors = similarity.select('*', rank().over(window).alias('rank')).filter(col('rank') <= K)\n",
    "    \n",
    "    # Define function to perform majority vote\n",
    "    def majority_vote(neighbors):\n",
    "        return Vectors.dense(collections.Counter([x[1] for x in neighbors]).most_common()[0][0])\n",
    "    \n",
    "    # Compute predicted label by majority vote\n",
    "    predicted_labels = k_neighbors.rdd \\\n",
    "                        .map(lambda x: (x.test_id, (x.distance, x.train_label[0]))) \\\n",
    "                        .groupByKey() \\\n",
    "                        .map(lambda x: (x[0], majority_vote(list(x[1])))).collect()\n",
    "    \n",
    "    # Return result\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "def prepare_data(actual_data, prediction_data, on='test_id'):\n",
    "    return actual_data.join(prediction_data, on).rdd \\\n",
    "                        .map(lambda x: (float(x.prediction[0]), float(x.test_label[0])))\n",
    "\n",
    "    \n",
    "def overall_report(actual_data, prediction_data):\n",
    "    # Calculate actual / predicted labels in rdd from\n",
    "    prediction_and_labels = prepare_data(actual_data, prediction_data)\n",
    "    \n",
    "    # Calculate actual / predicted labels in rdd from\n",
    "    metrics = MulticlassMetrics(prediction_and_labels)\n",
    "    \n",
    "    # Calculate overall level metrics\n",
    "    return sc.parallelize([(Vectors.dense(round(metrics.precision(), 3)), \n",
    "                            Vectors.dense(round(metrics.recall(), 3)), \n",
    "                            Vectors.dense(round(metrics.fMeasure(), 3)))]).toDF(['Precision', 'Recall', 'F-Score'])\n",
    "\n",
    "\n",
    "def classification_report(actual_data, prediction_data):\n",
    "    # Calculate actual / predicted labels in rdd from\n",
    "    prediction_and_labels = prepare_data(actual_data, prediction_data)\n",
    "    \n",
    "    # Calculate calculate class level metrics\n",
    "    metrics = MulticlassMetrics(prediction_and_labels)\n",
    "    classes = set(actual_data.rdd.map(lambda x: x.test_label[0]).collect())\n",
    "    results = [(Vectors.dense(float(c)),\n",
    "                Vectors.dense(round(metrics.precision(c), 3)), \n",
    "                Vectors.dense(round(metrics.recall(c), 3)), \n",
    "                Vectors.dense(round(metrics.fMeasure(c), 3))) for c in sorted(classes)]\n",
    "    return sc.parallelize(results).toDF(['Class', 'Precision', 'Recall', 'F-Score'])\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_data, test_data = get_preprocessed_data(INPUT_TRAIN_DATA, INPUT_TEST_DATA)\n",
    "    \n",
    "    # Broadcast training features and examples\n",
    "    train_features = sc.broadcast(train_data.rdd.map(lambda x: x.train_feature).collect())\n",
    "    train_labels = sc.broadcast(train_data.rdd.map(lambda x: x.train_label).collect())\n",
    "    \n",
    "    # Compute KNN predictions\n",
    "    predictions = make_prediction(train_data, test_data, K=K_NEAREST_NEIGHBORS)\n",
    "\n",
    "    # Create dataframe of prediction resutlts\n",
    "    prediction_data = sc.parallelize(predictions).toDF(['test_id', 'prediction'])\n",
    "    \n",
    "    # Calculate overall summary statistics\n",
    "    overall_metrics = overall_report(test_data, prediction_data)\n",
    "    overall_metrics.show()\n",
    "    \n",
    "    # Calculate class level statistics\n",
    "    classification_metrics = classification_report(test_data, prediction_data)\n",
    "    classification_metrics.show()\n",
    "    \n",
    "    # Save statistics to CSV\n",
    "    overall_metrics.toPandas().to_csv(\n",
    "        '{0}/overall_metrics{1}.csv'.format(OUTPUT_DIR, FILE_FLAG), index=None)\n",
    "    classification_metrics.toPandas().to_csv(\n",
    "        '{0}/classification_report{1}.csv'.format(OUTPUT_DIR, FILE_FLAG), index=None)\n",
    "    \n",
    "    print('Complete. Output saved to {}'.format(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+\n",
      "|Precision|Recall|F-Score|\n",
      "+---------+------+-------+\n",
      "|   [0.84]|[0.84]| [0.84]|\n",
      "+---------+------+-------+\n",
      "\n",
      "+-----+---------+-------+-------+\n",
      "|Class|Precision| Recall|F-Score|\n",
      "+-----+---------+-------+-------+\n",
      "|[0.0]|  [0.889]|  [1.0]|[0.941]|\n",
      "|[1.0]|  [0.875]|  [1.0]|[0.933]|\n",
      "|[2.0]|   [0.75]| [0.75]| [0.75]|\n",
      "|[3.0]|    [1.0]|[0.818]|  [0.9]|\n",
      "|[4.0]|  [0.833]|[0.714]|[0.769]|\n",
      "|[5.0]|  [0.833]|[0.714]|[0.769]|\n",
      "|[6.0]|  [0.889]|  [0.8]|[0.842]|\n",
      "|[7.0]|  [0.929]|[0.867]|[0.897]|\n",
      "|[8.0]|    [1.0]|  [0.5]|[0.667]|\n",
      "|[9.0]|  [0.625]|[0.909]|[0.741]|\n",
      "+-----+---------+-------+-------+\n",
      "\n",
      "Complete. Output saved to /Users/nicholasmoore/Google Drive/University/COMP5349 - Cloud Computing/Assessment\n",
      "CPU times: user 280 ms, sys: 65.1 ms, total: 346 ms\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
